FROM apache/airflow:3.0.0

USER root

# Install Java (needed for pyspark/spark-submit)
RUN apt-get update && \
    apt-get install -y --no-install-recommends default-jdk curl && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-arm64
ENV PATH="${JAVA_HOME}/bin:${PATH}"

USER airflow

# Install PySpark (includes spark-submit) and Airflow Spark provider
RUN pip install --no-cache-dir \
    pyspark==4.1.1 \
    apache-airflow-providers-apache-spark>=4.0.0 \
    apache-airflow-providers-openlineage>=2.0.0

# Download AWS SDK v2 bundle into PySpark's jars directory
# hadoop-aws is bundled with PySpark 4.1.1 (Hadoop 3.4.1)
# but the AWS SDK v2 bundle is still needed for S3A to work with MinIO.
RUN PYSPARK_JARS=$(python3 -c "import pyspark; import os; print(os.path.join(os.path.dirname(pyspark.__file__), 'jars'))") && \
    curl -fSL -o "${PYSPARK_JARS}/bundle-2.31.1.jar" \
    "https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.31.1/bundle-2.31.1.jar"
