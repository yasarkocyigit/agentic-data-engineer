# =============================================================
# AIRFLOW AGENTIC AI - Full Stack Docker Compose
# Services: Spark, MinIO, Marquez, Airflow
# =============================================================

services:
  # ---------------------------------------------------------
  # OPENCLAW API (FastAPI Backend)
  # ---------------------------------------------------------
  openclaw-api:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: openclaw_api
    ports:
      - "8000:8000"
    environment:
      - AIRFLOW_API_URL=http://airflow-webserver:8080
      - AIRFLOW_SA_USERNAME=admin
      - AIRFLOW_SA_PASSWORD=admin
      - GITEA_URL=${GITEA_URL:-http://gitea:3000}
      - GITEA_TOKEN=${GITEA_TOKEN:-}
      - GITEA_USERNAME=${GITEA_USERNAME:-admin}
      - GITEA_PASSWORD=${GITEA_PASSWORD:-admin}
      - MARQUEZ_URL=http://marquez:5000
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_ROOT_USER=${MINIO_ROOT_USER:-admin}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD:-admin123}
      - TRINO_HOST=trino
      - TRINO_PORT=8080
      - PG_HOST=host.docker.internal
      - PG_PORT=5433
      - PG_USER=${PG_USER:-postgres}
      - PG_PASSWORD=${PG_PASSWORD:-Gs+163264128}
      - PG_DATABASE=${PG_DATABASE:-controldb}
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_REMOTE_URL=sc://spark-connect:15002
      - SPARK_APP_NAME=openclaw-sql
      - SPARK_S3_ENDPOINT=http://minio:9000
      - SPARK_MASTER_UI_PUBLIC_URL=http://localhost:8082
      - SPARK_EXTRA_JARS=/workspace/jars/hadoop-aws-3.4.1.jar,/workspace/jars/bundle-2.31.1.jar
      - SPARK_DELTA_PACKAGE=io.delta:delta-spark_2.13:4.0.0
      - SPARK_EXTRA_PACKAGES=org.apache.iceberg:iceberg-spark-runtime-4.0_2.13:1.10.1,org.postgresql:postgresql:42.7.5
      - SPARK_ICEBERG_ENABLED=true
      - SPARK_ICEBERG_CATALOG=lakehouse
      - ICEBERG_CATALOG_URI=${ICEBERG_CATALOG_URI:-jdbc:postgresql://host.docker.internal:5433/controldb}
      - ICEBERG_CATALOG_USER=${ICEBERG_CATALOG_USER:-postgres}
      - ICEBERG_CATALOG_PASSWORD=${ICEBERG_CATALOG_PASSWORD:-Gs+163264128}
      - ICEBERG_WAREHOUSE=${ICEBERG_WAREHOUSE:-s3a://iceberg/warehouse/}
      - NOTEBOOK_SQL_ENGINE=spark
      - NOTEBOOK_USE_GATEWAY=false
      - SPARK_DEFAULT_DATABASE=default
      - NOTEBOOK_CLUSTER_PROFILES_JSON=${NOTEBOOK_CLUSTER_PROFILES_JSON:-}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./notebooks:/opt/airflow/notebooks
      - .:/workspace
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - airflow-webserver
      - trino
      - spark-master
      - spark-connect
      - marquez
      - minio
    restart: unless-stopped
    networks:
      - agentic-network

  # ---------------------------------------------------------
  # SPARK MASTER
  # ---------------------------------------------------------
  spark-master:
    image: apache/spark:4.0.1
    container_name: spark_master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8082:8080" # Spark Master UI (8082 to avoid conflict)
      - "7077:7077" # Spark Master port
    volumes:
      - ./notebooks:/opt/spark/work/notebooks
      - ./config:/opt/spark/work/config
      - ./jars:/opt/spark/extra-jars
    networks:
      - agentic-network

  # ---------------------------------------------------------
  # SPARK WORKER
  # ---------------------------------------------------------
  spark-worker:
    image: apache/spark:4.0.1
    container_name: spark_worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY:-2G}
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES:-2}
      - SPARK_WORKER_DIR=/tmp/spark-worker
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    volumes:
      - ./notebooks:/opt/spark/work/notebooks
      - ./config:/opt/spark/work/config
      - ./jars:/opt/spark/extra-jars
    networks:
      - agentic-network

  # ---------------------------------------------------------
  # SPARK CONNECT SERVER (Remote Notebook Compute)
  # ---------------------------------------------------------
  spark-connect:
    image: apache/spark:4.0.1
    container_name: spark_connect
    environment:
      - SPARK_NO_DAEMONIZE=true
      - HOME=/tmp
      - SPARK_JARS_IVY=/tmp/.ivy2
      - SPARK_SUBMIT_OPTS=-Divy.cache.dir=/tmp/.ivy2/cache -Divy.home=/tmp/.ivy2 -Duser.home=/tmp
    command: >
      /bin/bash -lc "/opt/spark/sbin/start-connect-server.sh
      --master spark://spark-master:7077
      --packages io.delta:delta-spark_2.13:4.0.0,org.apache.iceberg:iceberg-spark-runtime-4.0_2.13:1.10.1,org.postgresql:postgresql:42.7.5,org.apache.hadoop:hadoop-aws:3.4.1,software.amazon.awssdk:bundle:2.31.1
      --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
      --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
      --conf spark.sql.catalog.lakehouse=org.apache.iceberg.spark.SparkCatalog
      --conf spark.sql.catalog.lakehouse.type=jdbc
      --conf spark.sql.catalog.lakehouse.uri=${ICEBERG_CATALOG_URI:-jdbc:postgresql://host.docker.internal:5433/controldb}
      --conf spark.sql.catalog.lakehouse.jdbc.user=${ICEBERG_CATALOG_USER:-postgres}
      --conf spark.sql.catalog.lakehouse.jdbc.password=${ICEBERG_CATALOG_PASSWORD:-Gs+163264128}
      --conf spark.sql.catalog.lakehouse.warehouse=${ICEBERG_WAREHOUSE:-s3a://iceberg/warehouse/}
      --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
      --conf spark.hadoop.fs.s3a.path.style.access=true
      --conf spark.hadoop.fs.s3a.access.key=${MINIO_ROOT_USER:-admin}
      --conf spark.hadoop.fs.s3a.secret.key=${MINIO_ROOT_PASSWORD:-admin123}
      --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false
      --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    ports:
      - "15002:15002"
    depends_on:
      - spark-master
      - spark-worker
      - minio
    networks:
      - agentic-network

  # ---------------------------------------------------------
  # MINIO (S3-Compatible Object Storage for Delta Lake)
  # ---------------------------------------------------------
  minio:
    image: minio/minio:latest
    container_name: minio_lakehouse
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-admin123}
    ports:
      - "9000:9000" # MinIO API
      - "9001:9001" # MinIO Console
    volumes:
      - minio_data:/data
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - agentic-network

  # MinIO Client - Create initial buckets
  minio-setup:
    image: minio/mc:latest
    container_name: minio_setup
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c " /usr/bin/mc alias set myminio http://minio:9000 $${MINIO_ROOT_USER:-admin} $${MINIO_ROOT_PASSWORD:-admin123}; /usr/bin/mc mb myminio/bronze --ignore-existing; /usr/bin/mc mb myminio/silver --ignore-existing; /usr/bin/mc mb myminio/gold --ignore-existing; /usr/bin/mc mb myminio/warehouse --ignore-existing; /usr/bin/mc mb myminio/checkpoints --ignore-existing; /usr/bin/mc mb myminio/deltalake --ignore-existing; /usr/bin/mc mb myminio/iceberg --ignore-existing; echo 'Buckets created successfully!'; exit 0; "
    networks:
      - agentic-network

  # ---------------------------------------------------------
  # TRINO (SQL Query Engine for Iceberg)
  # ---------------------------------------------------------
  trino:
    image: trinodb/trino:latest
    container_name: trino_sql
    volumes:
      - ./trino/etc:/etc/trino
    ports:
      - "8083:8080"
    depends_on:
      minio:
        condition: service_healthy
    networks:
      - agentic-network

  # ---------------------------------------------------------
  # MARQUEZ (OpenLineage Data Lineage)
  # ---------------------------------------------------------
  marquez-db:
    image: postgres:15
    container_name: marquez_db
    environment:
      POSTGRES_USER: marquez
      POSTGRES_PASSWORD: marquez
      POSTGRES_DB: marquez
    volumes:
      - marquez_db_data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U marquez -d marquez" ]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - agentic-network

  marquez:
    image: marquezproject/marquez:latest
    platform: linux/amd64
    container_name: marquez_lineage
    environment:
      - MARQUEZ_PORT=5000
      - MARQUEZ_ADMIN_PORT=5001
      - POSTGRES_HOST=marquez-db
      - POSTGRES_PORT=5432
      - POSTGRES_DB=marquez
      - POSTGRES_USER=marquez
      - POSTGRES_PASSWORD=marquez
    ports:
      - "5002:5000" # Marquez API (5002 to avoid Mac AirPlay conflict)
      - "5003:5001" # Marquez Admin
    depends_on:
      marquez-db:
        condition: service_healthy
    networks:
      - agentic-network

  marquez-web:
    image: marquezproject/marquez-web:latest
    platform: linux/amd64
    container_name: marquez_web
    environment:
      - MARQUEZ_HOST=marquez
      - MARQUEZ_PORT=5000
      - WEB_PORT=3000
    ports:
      - "8085:3000" # Marquez Web UI (Changed from 3002)
    depends_on:
      - marquez
    networks:
      - agentic-network

  # ---------------------------------------------------------
  # AIRFLOW 3.x (Orchestration)
  # ---------------------------------------------------------
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow_init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_DB_CONN:-postgresql+psycopg2://postgres:changeme@host.docker.internal:5433/controldb}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY:-GENERATE_A_FERNET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY:-CHANGE_ME_IN_ENV_FILE}
      - AIRFLOW__OPENLINEAGE__TRANSPORT={"type":"http","url":"http://marquez:5000","endpoint":"api/v1/lineage"}
      - AIRFLOW__OPENLINEAGE__NAMESPACE=agentic-pipeline
    entrypoint: >
      /bin/bash -c " airflow db migrate; airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com; echo 'Airflow initialized!'; "
    networks:
      - agentic-network

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow_webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_DB_CONN:-postgresql+psycopg2://postgres:changeme@host.docker.internal:5433/controldb}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY:-GENERATE_A_FERNET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY:-CHANGE_ME_IN_ENV_FILE}
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.no_auth
      - AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS=admin:ADMIN
      - AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE=/opt/airflow/config/simple_auth_manager_passwords.json
      - AIRFLOW__API_AUTH__JWT_SECRET=${AIRFLOW_JWT_SECRET:-CHANGE_ME_IN_ENV_FILE}
      - AIRFLOW__OPENLINEAGE__TRANSPORT={"type":"http","url":"http://marquez:5000","endpoint":"api/v1/lineage"}
      - AIRFLOW__OPENLINEAGE__NAMESPACE=agentic-pipeline
    command: api-server
    ports:
      - "8081:8080" # Airflow UI
    volumes:
      - ./dags:/opt/airflow/dags
      - ./config:/opt/airflow/config
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    depends_on:
      - airflow-init
    restart: unless-stopped
    networks:
      - agentic-network

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow_scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_DB_CONN:-postgresql+psycopg2://postgres:changeme@host.docker.internal:5433/controldb}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY:-GENERATE_A_FERNET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY:-CHANGE_ME_IN_ENV_FILE}
      - AIRFLOW__CORE__INTERNAL_API_URL=http://airflow-webserver:8080
      - AIRFLOW__CORE__EXECUTION_API_SERVER_URL=http://airflow-webserver:8080/execution/
      - AIRFLOW__API_AUTH__JWT_SECRET=${AIRFLOW_JWT_SECRET:-CHANGE_ME_IN_ENV_FILE}
      - AIRFLOW__OPENLINEAGE__TRANSPORT={"type":"http","url":"http://marquez:5000","endpoint":"api/v1/lineage"}
      - AIRFLOW__OPENLINEAGE__NAMESPACE=agentic-pipeline
      - ICEBERG_CATALOG_URI=${ICEBERG_CATALOG_URI:-jdbc:postgresql://host.docker.internal:5433/controldb}
      - ICEBERG_CATALOG_USER=${ICEBERG_CATALOG_USER:-postgres}
      - ICEBERG_CATALOG_PASSWORD=${ICEBERG_CATALOG_PASSWORD:-Gs+163264128}
      - ICEBERG_WAREHOUSE=${ICEBERG_WAREHOUSE:-s3a://iceberg/warehouse/}
    command: scheduler
    volumes:
      - ./dags:/opt/airflow/dags
      - ./config:/opt/airflow/config
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./notebooks:/opt/airflow/notebooks
      - ./jars:/opt/spark/extra-jars
    depends_on:
      - airflow-init
      - marquez
    restart: unless-stopped
    networks:
      - agentic-network

  airflow-dag-processor:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow_dag_processor
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_DB_CONN:-postgresql+psycopg2://postgres:changeme@host.docker.internal:5433/controldb}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY:-GENERATE_A_FERNET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY:-CHANGE_ME_IN_ENV_FILE}
      - AIRFLOW__CORE__INTERNAL_API_URL=http://airflow-webserver:8080
      - AIRFLOW__CORE__EXECUTION_API_SERVER_URL=http://airflow-webserver:8080/execution/
      - AIRFLOW__API_AUTH__JWT_SECRET=${AIRFLOW_JWT_SECRET:-CHANGE_ME_IN_ENV_FILE}
      - AIRFLOW__OPENLINEAGE__TRANSPORT={"type":"http","url":"http://marquez:5000","endpoint":"api/v1/lineage"}
      - AIRFLOW__OPENLINEAGE__NAMESPACE=agentic-pipeline
    command: dag-processor
    volumes:
      - ./dags:/opt/airflow/dags
      - ./config:/opt/airflow/config
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    depends_on:
      - airflow-init
    restart: unless-stopped
    networks:
      - agentic-network

  # ---------------------------------------------------------
  # APACHE SUPERSET (Data Visualization)
  # ---------------------------------------------------------
  superset-redis:
    image: redis:7-alpine
    container_name: superset_redis
    networks:
      - agentic-network

  superset-init:
    build:
      context: .
      dockerfile: config/Dockerfile.superset
    platform: linux/amd64
    container_name: superset_init
    environment:
      - SUPERSET_CONFIG_PATH=/app/superset_config.py
      - SUPERSET_SECRET_KEY=openclaw-superset-secret-key-2026
    volumes:
      - ./config/superset_config.py:/app/superset_config.py:ro
      - ./config/superset_bootstrap.py:/app/superset_bootstrap.py:ro
      - superset_home:/app/superset_home
    command: >
      sh -c "superset db upgrade &&
             superset fab create-admin --username admin --firstname Admin --lastname User --email admin@example.com --password admin || true &&
             superset init &&
             python /app/superset_bootstrap.py &&
             echo 'Superset initialized!'"
    depends_on:
      superset-redis:
        condition: service_started
    networks:
      - agentic-network

  superset:
    build:
      context: .
      dockerfile: config/Dockerfile.superset
    platform: linux/amd64
    container_name: superset_app
    environment:
      - SUPERSET_CONFIG_PATH=/app/superset_config.py
      - SUPERSET_SECRET_KEY=openclaw-superset-secret-key-2026
    ports:
      - "8089:8088" # Superset UI
    volumes:
      - ./config/superset_config.py:/app/superset_config.py:ro
      - ./config/superset_bootstrap.py:/app/superset_bootstrap.py:ro
      - superset_home:/app/superset_home
    depends_on:
      - superset-init
    restart: unless-stopped
    networks:
      - agentic-network

  # ---------------------------------------------------------
  # GITEA (Self-hosted Git + CI/CD)
  # ---------------------------------------------------------
  gitea:
    image: gitea/gitea:latest
    container_name: gitea_server
    environment:
      - USER_UID=1000
      - USER_GID=1000
      - GITEA__database__DB_TYPE=sqlite3
      - GITEA__server__ROOT_URL=http://localhost:3030
      - GITEA__server__HTTP_PORT=3000
      - GITEA__actions__ENABLED=true
      - GITEA__security__INSTALL_LOCK=true
      - GITEA__service__DISABLE_REGISTRATION=true
    volumes:
      - gitea_data:/data
    ports:
      - "3030:3000" # Gitea Web UI
      - "2222:22" # Gitea SSH
    restart: unless-stopped
    networks:
      - agentic-network

  gitea-runner:
    image: gitea/act_runner:latest
    container_name: gitea_runner
    environment:
      - GITEA_INSTANCE_URL=http://gitea:3000
      - GITEA_RUNNER_REGISTRATION_TOKEN=UxSlNqbIwVoYWln9r8q6Jo6nRDK7gxR2Q8WTYm7L
      - GITEA_RUNNER_NAME=agentic-runner
      - GITEA_RUNNER_LABELS=ubuntu-latest:docker://node:20-bookworm,ubuntu-22.04:docker://node:20-bookworm
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - gitea
    restart: unless-stopped
    networks:
      - agentic-network

  # ---------------------------------------------------------
  # JUPYTER KERNEL GATEWAY (Interactive Notebook Execution)
  # ---------------------------------------------------------
  jupyter-kernel:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter_kernel_gw
    environment:
      - JUPYTER_GATEWAY_PORT=8888
      - SPARK_MASTER=spark://spark-master:7077
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
      - SPARK_OPTS=--packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER:-admin}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD:-admin123}
    command: >
      bash -c "pip install jupyter_kernel_gateway trino plotly pyarrow --quiet &&
               jupyter kernelgateway
               --KernelGatewayApp.ip=0.0.0.0
               --KernelGatewayApp.port=8888
               --KernelGatewayApp.allow_origin='*'
               --KernelGatewayApp.api='kernel_gateway.jupyter_websocket'
               --JupyterApp.answer_yes=true"
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work/notebooks
      - ./jars:/home/jovyan/work/jars
    depends_on:
      - spark-master
    restart: unless-stopped
    networks:
      - agentic-network

# ---------------------------------------------------------
# VOLUMES
# ---------------------------------------------------------
volumes:
  minio_data:
  marquez_db_data:
  superset_home:
  gitea_data:


networks:
  agentic-network:
    driver: bridge
